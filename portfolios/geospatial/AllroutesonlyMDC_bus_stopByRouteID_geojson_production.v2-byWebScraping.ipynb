{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get RouteID/RouteName information from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\andy\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.miamidade.gov/transportation-publicworks/routes.asp'\n",
    "page = requests.get(URL)\n",
    "\n",
    "routes_name = BeautifulSoup(page.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_ids = []\n",
    "for i in range(len(routes_name.find_all('option'))):\n",
    "    route_ids.append(str(routes_name.find_all('option')[i]).split('>')[0].split('value=')[1].replace('\"','').replace('#',''))\n",
    "    \n",
    "route_names = []\n",
    "for i in range(len(routes_name.find_all('option'))):\n",
    "    route_names.append(routes_name.find_all('option')[i].contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_names = pd.concat([pd.DataFrame(route_ids[1:]), pd.DataFrame(route_names[1:])], axis=1)\n",
    "route_names.columns = ['RouteID','RouteName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_names['RouteID'] = route_names['RouteID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by routeid\n",
    "route_names = route_names.sort_values(by='RouteID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\andy\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "URL2 = 'http://www.miamidade.gov/transit/WebServices/BusRouteDirections/?RouteID'\n",
    "page2 = requests.get(URL2)\n",
    "\n",
    "routes_info = BeautifulSoup(page2.content)\n",
    "routes_info = routes_info.find_all('routeid')\n",
    "\n",
    "\n",
    "routes_list = []\n",
    "for i in range(len(routes_info)):\n",
    "    routes_list.append(int(routes_info[i].contents[0]))\n",
    "    \n",
    "routes_list = list(routes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\andy\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "direction_info = BeautifulSoup(page2.content)\n",
    "direction_info = direction_info.find_all('direction')\n",
    "\n",
    "dir_list = []\n",
    "for i in range(len(direction_info)):\n",
    "    dir_list.append(direction_info[i].contents[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_dir_list = pd.concat([pd.DataFrame(routes_list),pd.DataFrame(dir_list)], axis=1)\n",
    "routes_dir_list.columns = ['RouteID','Direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_dir_list = routes_dir_list.sort_values(by = 'RouteID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes_dir_list = routes_dir_list.merge(route_names, on = 'RouteID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set route list as unique routeIDs for script below\n",
    "routes_list = routes_dir_list['RouteID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get stop information from web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#route = 77\n",
    "def stop_df_webscraping(routes_list):\n",
    "\n",
    "\n",
    "    for route in routes_list:\n",
    "        ## Looks like some routes, such as 39 is missing so just pass those routes\n",
    "        try:\n",
    "            URL = 'https://www.miamidade.gov/transit/mobile/appPages/MapBusStop/?RouteID='+str(route)+'&amp;Dir=&amp;StopID=&amp;Sequence=&amp;nearby=&amp;BusID=ALL&amp;src=mobile'\n",
    "            page = requests.get(URL)\n",
    "\n",
    "            soup = BeautifulSoup(page.content)\n",
    "            result = soup.find_all('script')\n",
    "\n",
    "            test = result[1].contents[0].split(';')\n",
    "            df = pd.DataFrame(test)\n",
    "            stops = df[df[0].str.contains('StopAttributes')]\n",
    "            stops = pd.DataFrame(stops[0].str.split('=',1).tolist(),\n",
    "                                             columns = ['flips','row'])\n",
    "\n",
    "            stops['clean'] = stops['row'].apply(lambda x:x.replace('\\r','').replace('\\n',''))\n",
    "            stops['final'] = stops['clean'].apply(lambda x:x.replace('{','').replace('}','').replace('\\'',''))\n",
    "\n",
    "            final = stops[0::2]\n",
    "            final2 = pd.DataFrame(final['final'].str.split(',').tolist())\n",
    "\n",
    "            for i in range(len(final2.columns)):\n",
    "                pd.DataFrame(final2[i].str.split(':').tolist())\n",
    "\n",
    "            final3 = [pd.DataFrame(final2[i].str.split(':').tolist()) for i in range(len(final2.columns))]\n",
    "            pdList = [final3[i] for i in range(len(final3))]\n",
    "\n",
    "            # List of your dataframes\n",
    "            new_df = pd.concat(pdList, axis=1)\n",
    "            new_col = list(new_df[0].values[0])\n",
    "            new_df = new_df[1]\n",
    "            new_df.columns = new_col\n",
    "\n",
    "            return new_df\n",
    "        \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\andy\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "pdList = []\n",
    "for route in routes_list:\n",
    "    pdList.append(stop_df_webscraping([route]))\n",
    "\n",
    "\n",
    "full_df = pd.concat(pdList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df.RouteID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get polyline (route multi-line) from webscraping\n",
    "\n",
    "since some of the routes are missing from Open Hub API, let's try to get routes from web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rt_df_webscraping(routes_list):\n",
    "\n",
    "\n",
    "    for route in routes_list:\n",
    "        ## Looks like some routes, such as 39 is missing so just pass those routes\n",
    "        try:\n",
    "\n",
    "            URL = 'https://www.miamidade.gov/transit/mobile/appPages/MapBusStop/?RouteID='+str(route)+'&amp;Dir=&amp;StopID=&amp;Sequence=&amp;nearby=&amp;BusID=ALL&amp;src=mobile'\n",
    "            page = requests.get(URL)\n",
    "\n",
    "            soup = BeautifulSoup(page.content)\n",
    "            result = soup.find_all('script')\n",
    "\n",
    "            test = result[1].contents[0].split(';')\n",
    "            df = pd.DataFrame(test)\n",
    "\n",
    "            routes = df[df[0].str.contains('polyline')]\n",
    "            routes = pd.DataFrame(routes[0].str.split('=',1).tolist(),columns = ['flips','row'])\n",
    "            #routes\n",
    "\n",
    "            #routes['clean'] = routes['row'].apply(lambda x:x.replace('\\r','').replace('\\n',''))\n",
    "\n",
    "            clean = []\n",
    "\n",
    "            for i in routes['row']:\n",
    "                try:\n",
    "                    clean.append(i.replace('\\r','').replace('\\n',''))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            #routes['final'] = routes['clean'].apply(lambda x:x.replace('{','').replace('}','').replace('\\'',''))\n",
    "\n",
    "            final = []\n",
    "\n",
    "            for i in clean:\n",
    "                try:\n",
    "                    final.append(i.replace('\\r','').replace('\\n',''))\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            final = pd.DataFrame(final)[0::2]\n",
    "            final = final.reset_index()\n",
    "            \n",
    "            for i in range(len(final)):\n",
    "                final[0] = final[0].apply(lambda x:x.replace('{','').replace(' type: \"polyline\"','').replace(',paths: ','').replace('}',''))\n",
    "                \n",
    "            final.columns = ['index','coord']\n",
    "            final['RouteID'] = route\n",
    "            \n",
    "            # remove coord that is empty    \n",
    "            for coord in range(len(final['coord'][final['RouteID'] == route])):\n",
    "                if len(final['coord'][final['RouteID'] == route][coord]) == 2:\n",
    "                    final.drop(coord, inplace = True)\n",
    "\n",
    "            final.reset_index(inplace = True, drop = True)\n",
    "\n",
    "            return final\n",
    "        \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\Users\\andy\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "pdList2 = []\n",
    "\n",
    "# routes_list = [11,51,77,277,119,120]  # If you want to select only 6 routes, use this. Other wise, ignore this\n",
    "routes_list = [11,51,77,277,119,120,112,9,27,3,8,22] # If you want to select only 6 routes plus expanding routes, use this. Other wise, ignore this\n",
    "routes_list = sorted(routes_list)  # Same as above\n",
    "\n",
    "for route in routes_list:\n",
    "    pdList2.append(rt_df_webscraping([route]))\n",
    "\n",
    "\n",
    "full_df2 = pd.concat(pdList2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df2 = full_df2.reset_index()\n",
    "full_df2.index = full_df2['level_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_route_webscrap(route):\n",
    "    feature_rt = {'type':'Feature',\n",
    "                   'properties':{},\n",
    "                   'geometry':{'type':'MultiLineString',\n",
    "                               'coordinates':[]}}\n",
    "    \n",
    "    feature_rt['properties'] = {'RouteID': route}\n",
    "    for i in range(len(full_df2[full_df2['RouteID']==route])):\n",
    "        feature_rt['geometry']['coordinates'].append(full_df2[full_df2['RouteID']==route]['coord'][i])\n",
    "    \n",
    "    return feature_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Error here, coord at index 9 is missing for some reason. Just ignore this part - it still works fine; I guess tis mistake by MDC.\n",
    "#full_df2[full_df2['RouteID'] == 112]\n",
    "#full_df2['coord'][full_df2['RouteID'] == 112][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make geojson version of routes from web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first = routes_list[0]\n",
    "#routes_selected = routes_list[1:]\n",
    "feature_col = {'type':'FeatureCollection', 'features':[]}\n",
    "\n",
    "for route in routes_list:\n",
    "    #for example, RouteID == 81 does not have any coordinate since this route is currently out of service\n",
    "    if len(full_df2[full_df2['RouteID']==route]) == 0:\n",
    "        continue\n",
    "    else:        \n",
    "    #feature_route_webscrap(first).append(feature_route_webscrap(i))\n",
    "        feature_col['features'].append(feature_route_webscrap(route))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### color codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_code = {\n",
    " # \"aliceblue\": \"#f0f8ff\",\n",
    " # \"antiquewhite\": \"#faebd7\",\n",
    " # \"aqua\": \"#00ffff\",\n",
    " # \"aquamarine\": \"#7fffd4\",\n",
    "  \"azure\": \"#f0ffff\",\n",
    "   \"brown\": \"#a52a2a\",\n",
    "  \"bisque\": \"#ffe4c4\",\n",
    "  \"black\": \"#000000\",\n",
    "  \"darkgrey\": \"#a9a9a9\",\n",
    "  \"blue\": \"#0000ff\",\n",
    "  \"blueviolet\": \"#8a2be2\",\n",
    "  \"beige\": \"#f5f5dc\",\n",
    "    \"blanchedalmond\": \"#ffebcd\",\n",
    "  \"burlywood\": \"#deb887\",\n",
    "  \"cadetblue\": \"#5f9ea0\",\n",
    "  \"chartreuse\": \"#7fff00\",\n",
    "  \"chocolate\": \"#d2691e\",\n",
    "  \"coral\": \"#ff7f50\",\n",
    "  \"cornflowerblue\": \"#6495ed\",\n",
    "  \"cornsilk\": \"#fff8dc\",\n",
    "  \"crimson\": \"#dc143c\",\n",
    "  \"cyan\": \"#00ffff\",\n",
    "  \"darkblue\": \"#00008b\",\n",
    "  \"darkcyan\": \"#008b8b\",\n",
    "  \"darkgoldenrod\": \"#b8860b\",\n",
    "  \"darkgray\": \"#a9a9a9\",\n",
    "  \"darkgreen\": \"#006400\",\n",
    "  \"darkkhaki\": \"#bdb76b\",\n",
    "  \"darkmagenta\": \"#8b008b\",\n",
    "  \"darkolivegreen\": \"#556b2f\",\n",
    "  \"darkorange\": \"#ff8c00\",\n",
    "  \"darkorchid\": \"#9932cc\",\n",
    "  \"darkred\": \"#8b0000\",\n",
    "  \"darksalmon\": \"#e9967a\",\n",
    "  \"darkseagreen\": \"#8fbc8f\",\n",
    "  \"darkslateblue\": \"#483d8b\",\n",
    "  \"darkslategray\": \"#2f4f4f\",\n",
    "  \"darkslategrey\": \"#2f4f4f\",\n",
    "  \"darkturquoise\": \"#00ced1\",\n",
    "  \"darkviolet\": \"#9400d3\",\n",
    "  \"deeppink\": \"#ff1493\",\n",
    "  \"deepskyblue\": \"#00bfff\",\n",
    "  \"dimgray\": \"#696969\",\n",
    "  \"dimgrey\": \"#696969\",\n",
    "  \"dodgerblue\": \"#1e90ff\",\n",
    "  \"firebrick\": \"#b22222\",\n",
    "  \"floralwhite\": \"#fffaf0\",\n",
    "  \"forestgreen\": \"#228b22\",\n",
    "  \"fuchsia\": \"#ff00ff\",\n",
    "  \"gainsboro\": \"#dcdcdc\",\n",
    "  \"ghostwhite\": \"#f8f8ff\",\n",
    "  \"goldenrod\": \"#daa520\",\n",
    "  \"gold\": \"#ffd700\",\n",
    "  \"gray\": \"#808080\",\n",
    "  \"green\": \"#008000\",\n",
    "  \"greenyellow\": \"#adff2f\",\n",
    "  \"grey\": \"#808080\",\n",
    "  \"honeydew\": \"#f0fff0\",\n",
    "  \"hotpink\": \"#ff69b4\",\n",
    "  \"indianred\": \"#cd5c5c\",\n",
    "  \"indigo\": \"#4b0082\",\n",
    "  \"ivory\": \"#fffff0\",\n",
    "  \"khaki\": \"#f0e68c\",\n",
    "  \"lavenderblush\": \"#fff0f5\",\n",
    "  \"lavender\": \"#e6e6fa\",\n",
    "  \"lawngreen\": \"#7cfc00\",\n",
    "  \"lemonchiffon\": \"#fffacd\",\n",
    "  \"lightblue\": \"#add8e6\",\n",
    "  \"lightcoral\": \"#f08080\",\n",
    "  \"lightcyan\": \"#e0ffff\",\n",
    "  \"lightgoldenrodyellow\": \"#fafad2\",\n",
    "  \"lightgray\": \"#d3d3d3\",\n",
    "  \"lightgreen\": \"#90ee90\",\n",
    "  \"lightgrey\": \"#d3d3d3\",\n",
    "  \"lightpink\": \"#ffb6c1\",\n",
    "  \"lightsalmon\": \"#ffa07a\",\n",
    "  \"lightseagreen\": \"#20b2aa\",\n",
    "  \"lightskyblue\": \"#87cefa\",\n",
    "  \"lightslategray\": \"#778899\",\n",
    "  \"lightslategrey\": \"#778899\",\n",
    "  \"lightsteelblue\": \"#b0c4de\",\n",
    "  \"lightyellow\": \"#ffffe0\",\n",
    "  \"lime\": \"#00ff00\",\n",
    "  \"limegreen\": \"#32cd32\",\n",
    "  \"linen\": \"#faf0e6\",\n",
    "  \"magenta\": \"#ff00ff\",\n",
    "  \"maroon\": \"#800000\",\n",
    "  \"mediumaquamarine\": \"#66cdaa\",\n",
    "  \"mediumblue\": \"#0000cd\",\n",
    "  \"mediumorchid\": \"#ba55d3\",\n",
    "  \"mediumpurple\": \"#9370db\",\n",
    "  \"mediumseagreen\": \"#3cb371\",\n",
    "  \"mediumslateblue\": \"#7b68ee\",\n",
    "  \"mediumspringgreen\": \"#00fa9a\",\n",
    "  \"mediumturquoise\": \"#48d1cc\",\n",
    "  \"mediumvioletred\": \"#c71585\",\n",
    "  \"midnightblue\": \"#191970\",\n",
    "  \"mintcream\": \"#f5fffa\",\n",
    "  \"mistyrose\": \"#ffe4e1\",\n",
    "  \"moccasin\": \"#ffe4b5\",\n",
    "  \"navajowhite\": \"#ffdead\",\n",
    "  \"navy\": \"#000080\",\n",
    "  \"oldlace\": \"#fdf5e6\",\n",
    "  \"olive\": \"#808000\",\n",
    "  \"olivedrab\": \"#6b8e23\",\n",
    "  \"orange\": \"#ffa500\",\n",
    "  \"orangered\": \"#ff4500\",\n",
    "  \"orchid\": \"#da70d6\",\n",
    "  \"palegoldenrod\": \"#eee8aa\",\n",
    "  \"palegreen\": \"#98fb98\",\n",
    "  \"paleturquoise\": \"#afeeee\",\n",
    "  \"palevioletred\": \"#db7093\",\n",
    "  \"papayawhip\": \"#ffefd5\",\n",
    "  \"peachpuff\": \"#ffdab9\",\n",
    "  \"peru\": \"#cd853f\",\n",
    "  \"pink\": \"#ffc0cb\",\n",
    "  \"plum\": \"#dda0dd\",\n",
    "  \"powderblue\": \"#b0e0e6\",\n",
    "  \"purple\": \"#800080\",\n",
    "  \"rebeccapurple\": \"#663399\",\n",
    "  \"red\": \"#ff0000\",\n",
    "  \"rosybrown\": \"#bc8f8f\",\n",
    "  \"royalblue\": \"#4169e1\",\n",
    "  \"saddlebrown\": \"#8b4513\",\n",
    "  \"salmon\": \"#fa8072\",\n",
    "  \"sandybrown\": \"#f4a460\",\n",
    "  \"seagreen\": \"#2e8b57\",\n",
    "  \"seashell\": \"#fff5ee\",\n",
    "  \"sienna\": \"#a0522d\",\n",
    "  \"silver\": \"#c0c0c0\",\n",
    "  \"skyblue\": \"#87ceeb\",\n",
    "  \"slateblue\": \"#6a5acd\",\n",
    "  \"slategray\": \"#708090\",\n",
    "  \"slategrey\": \"#708090\",\n",
    "  \"snow\": \"#fffafa\",\n",
    "  \"springgreen\": \"#00ff7f\",\n",
    "  \"steelblue\": \"#4682b4\",\n",
    "  \"tan\": \"#d2b48c\",\n",
    "  \"teal\": \"#008080\",\n",
    "  \"thistle\": \"#d8bfd8\",\n",
    "  \"tomato\": \"#ff6347\",\n",
    "  \"turquoise\": \"#40e0d0\",\n",
    "  \"violet\": \"#ee82ee\",\n",
    "  \"wheat\": \"#f5deb3\",\n",
    "  \"white\": \"#ffffff\",\n",
    "  \"whitesmoke\": \"#f5f5f5\",\n",
    "  \"yellow\": \"#ffff00\",\n",
    "  \"yellowgreen\": \"#9acd32\"\n",
    "}\n",
    "\n",
    "colors = []\n",
    "for i in color_code:\n",
    "    colors.append(color_code[i])\n",
    "    \n",
    "#fill_color = ['red','green','blue','yellow','silver','black']\n",
    "fill_color = colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply color for each route\n",
    "for i in range(len(feature_col['features'])):\n",
    "    feature_col['features'][i]['properties']['stroke'] = fill_color[i]\n",
    "    feature_col['features'][i]['properties']['stroke-width'] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try to think about how to get MultiLineString from Webscraping but I think it is really difficult. Remember that each point is not continous\n",
    "### There are lines that are continous with points but remember that lines are connected. So route 77, for examples might have multiple lines (3? 5?) with multiple points in each line\n",
    "### Therefore, you really have to just cut for each PolyLine coordinates and consider each PolyLine as different coordinates (total 2 coordinates with multiple coords in each)\n",
    "### Refer to geojson.io randering on MultiStringLine to see how it works\n",
    "\n",
    "### Thankfully, we have API from Miami dade so use that for now. Maybe I don't need to do web scraping for all of these routes. I think OpenHub API keeps updating anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get StopInformation from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://opendata.arcgis.com/datasets/021adadcf6854f59852ff4652ad90c11_0.geojson\") as url:\n",
    "    stops_df = json.loads(url.read().decode())\n",
    "    #print(route_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "stop_api = json_normalize(stops_df['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry.coordinates</th>\n",
       "      <th>geometry.type</th>\n",
       "      <th>properties.ACCESSIBLE</th>\n",
       "      <th>properties.BENCHES</th>\n",
       "      <th>properties.CROSS_ST</th>\n",
       "      <th>properties.DIRECTION</th>\n",
       "      <th>properties.FID</th>\n",
       "      <th>properties.LAT</th>\n",
       "      <th>properties.LON</th>\n",
       "      <th>properties.MAIN_ST</th>\n",
       "      <th>properties.POINT_X</th>\n",
       "      <th>properties.POINT_Y</th>\n",
       "      <th>properties.SHELTERS</th>\n",
       "      <th>properties.STOPID</th>\n",
       "      <th>properties.STOP_CORNR</th>\n",
       "      <th>properties.TRASH_CAN</th>\n",
       "      <th>properties.UPDTE_DATE</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>[-80.21202569199994, 25.922776722000037]</td>\n",
       "      <td>Point</td>\n",
       "      <td>Y</td>\n",
       "      <td>6</td>\n",
       "      <td>(East lot)</td>\n",
       "      <td>E</td>\n",
       "      <td>7850</td>\n",
       "      <td>25.92277</td>\n",
       "      <td>-80.21202</td>\n",
       "      <td>Golden Glades P&amp;R</td>\n",
       "      <td>915151.948</td>\n",
       "      <td>578400.31</td>\n",
       "      <td>0</td>\n",
       "      <td>10378</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>10/17/2016 12:1</td>\n",
       "      <td>Feature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          geometry.coordinates geometry.type  \\\n",
       "7849  [-80.21202569199994, 25.922776722000037]         Point   \n",
       "\n",
       "     properties.ACCESSIBLE  properties.BENCHES properties.CROSS_ST  \\\n",
       "7849                     Y                   6          (East lot)   \n",
       "\n",
       "     properties.DIRECTION  properties.FID  properties.LAT  properties.LON  \\\n",
       "7849                    E            7850        25.92277       -80.21202   \n",
       "\n",
       "     properties.MAIN_ST  properties.POINT_X  properties.POINT_Y  \\\n",
       "7849  Golden Glades P&R          915151.948           578400.31   \n",
       "\n",
       "      properties.SHELTERS  properties.STOPID properties.STOP_CORNR  \\\n",
       "7849                    0              10378                     T   \n",
       "\n",
       "      properties.TRASH_CAN properties.UPDTE_DATE     type  \n",
       "7849                     0       10/17/2016 12:1  Feature  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_api[stop_api['properties.STOPID'] == 10378]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry.coordinates</th>\n",
       "      <th>geometry.type</th>\n",
       "      <th>properties.ACCESSIBLE</th>\n",
       "      <th>properties.BENCHES</th>\n",
       "      <th>properties.CROSS_ST</th>\n",
       "      <th>properties.DIRECTION</th>\n",
       "      <th>properties.FID</th>\n",
       "      <th>properties.LAT</th>\n",
       "      <th>properties.LON</th>\n",
       "      <th>properties.MAIN_ST</th>\n",
       "      <th>properties.POINT_X</th>\n",
       "      <th>properties.POINT_Y</th>\n",
       "      <th>properties.SHELTERS</th>\n",
       "      <th>properties.STOPID</th>\n",
       "      <th>properties.STOP_CORNR</th>\n",
       "      <th>properties.TRASH_CAN</th>\n",
       "      <th>properties.UPDTE_DATE</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-80.21506969099994, 25.92080072300007]</td>\n",
       "      <td>Point</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "      <td>(West lot)</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>25.92080</td>\n",
       "      <td>-80.21507</td>\n",
       "      <td>Golden Glades P&amp;R</td>\n",
       "      <td>914155.736</td>\n",
       "      <td>577676.084</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>9/20/2016 13:13</td>\n",
       "      <td>Feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7849</th>\n",
       "      <td>[-80.21202569199994, 25.922776722000037]</td>\n",
       "      <td>Point</td>\n",
       "      <td>Y</td>\n",
       "      <td>6</td>\n",
       "      <td>(East lot)</td>\n",
       "      <td>E</td>\n",
       "      <td>7850</td>\n",
       "      <td>25.92277</td>\n",
       "      <td>-80.21202</td>\n",
       "      <td>Golden Glades P&amp;R</td>\n",
       "      <td>915151.948</td>\n",
       "      <td>578400.310</td>\n",
       "      <td>0</td>\n",
       "      <td>10378</td>\n",
       "      <td>T</td>\n",
       "      <td>0</td>\n",
       "      <td>10/17/2016 12:1</td>\n",
       "      <td>Feature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          geometry.coordinates geometry.type  \\\n",
       "1      [-80.21506969099994, 25.92080072300007]         Point   \n",
       "7849  [-80.21202569199994, 25.922776722000037]         Point   \n",
       "\n",
       "     properties.ACCESSIBLE  properties.BENCHES properties.CROSS_ST  \\\n",
       "1                        N                   2          (West lot)   \n",
       "7849                     Y                   6          (East lot)   \n",
       "\n",
       "     properties.DIRECTION  properties.FID  properties.LAT  properties.LON  \\\n",
       "1                       T               2        25.92080       -80.21507   \n",
       "7849                    E            7850        25.92277       -80.21202   \n",
       "\n",
       "     properties.MAIN_ST  properties.POINT_X  properties.POINT_Y  \\\n",
       "1     Golden Glades P&R          914155.736          577676.084   \n",
       "7849  Golden Glades P&R          915151.948          578400.310   \n",
       "\n",
       "      properties.SHELTERS  properties.STOPID properties.STOP_CORNR  \\\n",
       "1                       1                156                     T   \n",
       "7849                    0              10378                     T   \n",
       "\n",
       "      properties.TRASH_CAN properties.UPDTE_DATE     type  \n",
       "1                        2       9/20/2016 13:13  Feature  \n",
       "7849                     0       10/17/2016 12:1  Feature  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_api[stop_api['properties.MAIN_ST'] == 'Golden Glades P&R']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7536"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_df['StopID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that API provides more stops tan from web-scraping but it means 480 stops are currently not available in real-time\n",
    "len(stop_api['properties.STOPID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_api['properties.STOPID'] = stop_api['properties.STOPID'].astype('int')\n",
    "full_df['StopID'] = full_df['StopID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['RouteID'] = full_df['RouteID'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge StopInfo from API and webscraping to get StopID/RouteID from webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = full_df\n",
    "merged_df = stop_api.merge(full_df, left_on = 'properties.STOPID', right_on = 'StopID')\n",
    "# if stop_api doesn't have some of the stops from full_df and vice versa, just use full_df and don't merge at all.\n",
    "# Hmm, I think I noticed that stops that exist in full_df don't exist in stop_api.. figure out why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['geometry.coordinates', 'geometry.type', 'properties.ACCESSIBLE',\n",
       "       'properties.BENCHES', 'properties.CROSS_ST', 'properties.DIRECTION',\n",
       "       'properties.FID', 'properties.LAT', 'properties.LON',\n",
       "       'properties.MAIN_ST', 'properties.POINT_X', 'properties.POINT_Y',\n",
       "       'properties.SHELTERS', 'properties.STOPID', 'properties.STOP_CORNR',\n",
       "       'properties.TRASH_CAN', 'properties.UPDTE_DATE', 'type', ' MarkerType',\n",
       "       'Marker', 'StopID', 'StopName', 'StopNameURL', 'Sequence', 'RouteID',\n",
       "       'Dir', 'StopLat', 'StopLong'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns\n",
    "# Maybe use Properties.LAT/LON from stop_api instead of StopLat/Long from full_df..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = merged_df[['properties.STOPID', 'StopName','RouteID','properties.DIRECTION','properties.LAT','properties.LON','StopLat','StopLong']]\n",
    "final_df.columns = ['StopID', 'StopName','RouteID','Direction','Lat','Lon','StopLat','StopLong']\n",
    "# final_df = merged_df[['StopID', 'StopName','RouteID','Dir','StopLat','StopLong']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\andy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "final_df['StopID'] = final_df['StopID'].astype('int64')\n",
    "final_df['RouteID'] = final_df['RouteID'].astype('int64')\n",
    "final_df['StopLat'] = final_df['StopLat'].astype('float')\n",
    "final_df['StopLong'] = final_df['StopLong'].astype('float')\n",
    "final_df['Lat'] = final_df['Lat'].astype('float')\n",
    "final_df['Lon'] = final_df['Lon'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_polygon = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_polygon.RouteID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv('route_stop_mdc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw ploygon using coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import geog\n",
    "#import shapely\n",
    "import shapely.geometry\n",
    "\n",
    "def coord_to_polygon(stop_polygon):\n",
    "    polygon_list = []\n",
    "    for i in range(len(stop_polygon)):\n",
    "        p = shapely.geometry.Point(stop_polygon[['StopLong','StopLat']].values[i].astype('float'))\n",
    "        n_points = 20\n",
    "        d = 10 # meters\n",
    "        angles = np.linspace(0, 360, n_points)\n",
    "        polygon = geog.propagate(p, angles, d)\n",
    "    #    polygon_list.append(json.dumps(shapely.geometry.mapping(shapely.geometry.Polygon(polygon))))\n",
    "        polygon_list.append(polygon.tolist())\n",
    "        \n",
    "    return polygon_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_list = [11,51,77,277,119,120]   ## use this if you want just 6 routes\n",
    "filtered_list = [11,51,77,277,119,120,112,9,27,3,8,22]   ## use this if you want just 6 routes + expanding routes\n",
    "filtered_list = sorted(filtered_list)  ## same as above -- need this to sort the route in ascending order\n",
    "\n",
    "# filtered_list = stop_polygon['RouteID'].unique().tolist()  ## use this if you want ALL routes\n",
    "# filtered_list = routes_list ## use this if you want ALL routes\n",
    "\n",
    "# filtered_list = sorted(filtered_list) ## need this to sort the route in ascending order\n",
    "filtered_df = stop_polygon[stop_polygon['RouteID'].isin(filtered_list)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.sort_values(by='RouteID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = coord_to_polygon(stop_polygon=filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polyList = []\n",
    "# filtered_list = [11,51,77,277,119,120]\n",
    "\n",
    "# for i in filtered_list:\n",
    "#     filtered_df = stop_polygon[stop_polygon.RouteID == i]\n",
    "#     polyList.append(coord_to_polygon(stop_polygon=filtered_df))\n",
    "\n",
    "# #full_df = pd.concat(pdList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting df into GeoJson coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_geojson_point(df, properties, lat='latitude', lon='longitude'):\n",
    "    geojson = {'type':'FeatureCollection', 'features':[]}\n",
    "    for _, row in df.iterrows():\n",
    "        feature = {'type':'Feature',\n",
    "                   'properties':{},\n",
    "                   'geometry':{'type':'Point',\n",
    "                               'coordinates':[]}}\n",
    "        feature['geometry']['coordinates'] = [row[lon],row[lat]]\n",
    "        for prop in properties:\n",
    "            feature['properties'][prop] = row[prop]\n",
    "        geojson['features'].append(feature)\n",
    "    return geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df_to_geojson_point(stop_polygon, ['RouteID','StopName','Direction'], lat='StopLat', lon='StopLong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = json.dumps(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting df into GeoJson polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_geojson_polygon(df, properties, lat='latitude', lon='longitude'):\n",
    "    geojson = {'type':'FeatureCollection', 'features':[]}\n",
    "    for _, row in df.iterrows():\n",
    "        feature = {'type':'Feature',\n",
    "                   'properties':{},\n",
    "                   'geometry':{'type':'Polygon',\n",
    "                               'coordinates':[]}}\n",
    "        feature['geometry']['coordinates'] = [[row[lon],row[lat]]]\n",
    "        for prop in properties:\n",
    "            feature['properties'][prop] = row[prop]\n",
    "        geojson['features'].append(feature)\n",
    "    return geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fill color for each route differently\n",
    "for i,j in zip(filtered_list,fill_color):\n",
    "    filtered_df.loc[filtered_df['RouteID'] == i,'fill'] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2 = df_to_geojson_polygon(filtered_df, ['RouteID','StopName','Direction', 'StopID', 'fill'], lat='StopLat', lon='StopLong')\n",
    "##Without fill just to make 1826 map look prettier\n",
    "test2 = df_to_geojson_polygon(filtered_df, ['RouteID','StopName','Direction', 'StopID'], lat='StopLat', lon='StopLong')\n",
    "# test2 = df_to_geojson_polygon(filtered_df, ['RouteID','StopName','StopID', 'fill'], lat='StopLat', lon='StopLong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test2['features'][0]['geometry']['type'] = 'Polygon'\n",
    "for i in range(len(polygons)):\n",
    "    test2['features'][i]['geometry']['coordinates'] = [polygons[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('6routes.geojson', 'w', encoding='utf-8') as f: # or, maybe use .geojson ?\n",
    "#     json.dump(test2, f, ensure_ascii=False, indent=4) # Hmmm.. didn't know dump and dumps are different! Maybe I should do dumps instead in order to wrap with double colons \"\" instead of ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "t = json.dumps(feature_col).replace('\"[','[').replace('\"]',']').replace(']\"',']')\n",
    "\n",
    "with open('routes_webscraping.json', 'w') as f: # or, maybe use .geojson ?\n",
    "    f.write(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_web = json.loads(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct polyline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tt = geojson\n",
    "\n",
    "# for i in range(len(tt['features'])):\n",
    "#     for j in range(len(tt['features'][i]['geometry']['coordinates'])):\n",
    "#         for k in range(len(tt['features'][i]['geometry']['coordinates'][j])):\n",
    "#             for l in range(len(tt['features'][i]['geometry']['coordinates'][j][k])):   \n",
    "#                 tt['features'][i]['geometry']['coordinates'][j][k][l] = float(tt['features'][i]['geometry']['coordinates'][j][k][l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shapely\n",
    "# from shapely.geometry import MultiLineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(tt['features'])):\n",
    "#     try:\n",
    "#         print(MultiLineString(tt['features'][i]['geometry']['coordinates']))\n",
    "#     except:\n",
    "#         print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(geojson['features'])):\n",
    "#     try:\n",
    "#         print(MultiLineString(geojson['features'][i]['geometry']['coordinates']))\n",
    "#     except:\n",
    "#         print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(geojson['features'])):\n",
    "#     MultiLineString(geojson['features'][100]['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(geojson['features'])):\n",
    "#     print(i,len(geojson['features'][i]['geometry']['coordinates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLineString(geojson['features'][80]['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import geopandas as gpd\n",
    "# test3 = gpd.GeoDataFrame.from_features(route_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = MultiLineString(geojson['features'][80]['geometry']['coordinates'])  ## Change this into value and put it into column.. and then create GeoPandas features dataframe??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLineString(geojson['features'][80]['geometry']['coordinates']).buffer(0.5)\n",
    "## So I guess I just need to manually convert this back to GeoPandas format! Just like the one on the left!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoDataFrame(geojson['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLineString(geojson['features'][0]['geometry']['coordinates'])\n",
    "\n",
    "# find a way to convert this to just value.. something like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e = []\n",
    "# for i in range(len(geojson['features'])):\n",
    "#     try:\n",
    "#         e.append(MultiLineString(geojson['features'][i]['geometry']['coordinates']))\n",
    "#     except:\n",
    "#         print(i,'error')\n",
    "    \n",
    "# if below doesn't work, find a way to get printed version of MultiLiestring and put that into e!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('error_55.geojson','w') as f:\n",
    "#     json.dump(geojson['features'][53], f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiLineString(route_web['features'][53]['geometry']['coordinates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpd.GeoSeries(e[0].buffer(0.01)).to_json()  # Great! Use for loop to make column for each features and coordinates and make this as dataframe column just like the one in the left .ipyn file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(MultiLineString(geojson['features'][0]['geometry']['coordinates']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3.crs = {'init': 'epsg:4326'}\n",
    "#test3.crs = {'init': 'epsg:3174'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpr_gdf = test3.to_crs({'init': 'epsg:3174'})  # convert degrees into meters\n",
    "buffer_length_in_meters = (1 * 100) * 1.60934   # convert into 1 (1000 m * 1.60934) mile from meters\n",
    "cpr_gdf['geometry'] = cpr_gdf.geometry.buffer(buffer_length_in_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpr_gdf = cpr_gdf.to_crs({'init': 'epsg:4326'})  # convert geometry with 1 mile buffer back to degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpr_gdf.geometry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#line_poly = gpd.GeoSeries(test3.geometry.buffer(0.003)).to_json()   # thicker\n",
    "#line_poly = gpd.GeoSeries(test3.geometry.buffer(0.0013)).to_json() # slim (ideal)\n",
    "#line_poly = gpd.GeoSeries(test3.geometry.buffer(0.0001)).to_json() # request\n",
    "line_poly = gpd.GeoSeries(test3.geometry.buffer(0.00005)).to_json() # request2 --- this is the one we used for off-peak (correct one)\n",
    "#line_poly = gpd.GeoSeries(test3.geometry.buffer(0.02)).to_json()  # very thick\n",
    "\n",
    "# line_poly = gpd.GeoSeries(cpr_gdf.geometry.buffer(5280)).to_json() # 1 mile? (don't use this wrong)\n",
    "\n",
    "# line_poly = gpd.GeoSeries(cpr_gdf.geometry).to_json() # 1 mile (we used this for Kai)\n",
    "line_poly = json.loads(line_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test3['LINEABBR'] = test3['LINEABBR'].astype('int')\n",
    "test3['RouteID'] = test3['RouteID'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select routeIds from filtered_list\n",
    "line_poly_route_ids = test3['RouteID'].unique().tolist()\n",
    "\n",
    "\n",
    "# for i in range(len(filtered_list)):\n",
    "#     for j in filtered_list:\n",
    "#         line_poly['features'][i]['properties']['routes'] = j\n",
    "        \n",
    "# for i,j in enumerate(filtered_list):\n",
    "#     line_poly['features'][i]['properties']['routes'] = j\n",
    "    \n",
    "for i,j in enumerate(line_poly_route_ids):\n",
    "    line_poly['features'][i]['properties']['routes'] = j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Stops with PolyRoutes and line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_stops(stop_df):\n",
    "    for i in range(len(stop_df['features'])):\n",
    "        route_web['features'].append(stop_df['features'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_stops(test2)  # append each feature in polygon stops  -- for 1 mile stop, you don't need stops. don't add them (2020-09-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "append_stops(line_poly) # append each feature in polyline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph in folium with polyline and routes only for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# from IPython.display import display\n",
    "\n",
    "#gjson = hidro.to_crs(epsg='4326').to_json()\n",
    "# mapa = folium.Map([25.67, -81.866667],\n",
    "#                   zoom_start=4,\n",
    "#                   tiles='cartodbpositron')\n",
    "# looks like this gjson is working by each route only (not working if I put everything in one shot)\n",
    "# Maybe I should do the same thing for stops in polygon..? by stops for each route..?\n",
    "# Also, why can't I change the color for each route..? Find a reason!\n",
    "# Lastly, find if you can graph everything in just 1 geojson fil (it worked in local machine but why not in Jupyter..? Weird)\n",
    "\n",
    "#2020-07-01\n",
    "# For map, maybe I can add parameters that users choose in the map -- diameter for stops and buffer size for route (polygon size) , \n",
    "## routeID, users (meaning show all the markers/icons that are within the stops) who are within Stops, users who are within Route, users who are not within stops, users who are not within Route and etc\n",
    "## Also, if I can, try to add Bus image as icon/marker for bus stops (at the center of the diameter for stop polygon)\n",
    "\n",
    "# points = folium.features.GeoJson(gjson6)\n",
    "\n",
    "# mapa.add_child(points)\n",
    "\n",
    "# display(mapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export RouteLines and Polygons around it only\n",
    "# with open('route120_polylines_routes.json', 'w', encoding='utf-8') as f: # or, maybe use .geojson ?\n",
    "#     json.dump(route_web, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import github3\n",
    "# ghapi = github3.login('andy.sy.hwang@gmail.com', password='Qlqjs1123!') #github3.GitHub() \n",
    "# files = {'route120_polylines_routes.json': {'content': json.dumps(route_web)}} #json.dumps(geojson)\n",
    "# gist = ghapi.create_gist('', files, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRONG!!\n",
    "# final = test2\n",
    "# final['features'].append(geojson['features'])\n",
    "# final['features'].append(line_poly['features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the result into geojson/json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'polylines_6routes_expandingroutes_includingstops_buffer(0.00005).geojson'\n",
    "with open(name, 'w', encoding='utf-8') as f: # or, maybe use .geojson ?\n",
    "    json.dump(route_web, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export polygon stops only\n",
    "with open('MiamiRoute1mile.geojson', 'w', encoding='utf-8') as f: # or, maybe use .geojson ?\n",
    "    json.dump(route_web, f, ensure_ascii=False, indent=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export polylines only\n",
    "with open('polylines_6routes_expandingroutes buffer(0.00005).json', 'w', encoding='utf-8') as f: # or, maybe use .geojson ?\n",
    "    json.dump(line_poly, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rander map in gist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github3\n",
    "ghapi = github3.login('andy.sy.hwang@gmail.com', password='') #github3.GitHub() \n",
    "files = {name: {'content': json.dumps(route_web)}} #json.dumps(geojson)\n",
    "gist = ghapi.create_gist('', files, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github3\n",
    "ghapi = github3.login('andy.sy.hwang@gmail.com', password='') #github3.GitHub() \n",
    "files = {'all_realtime_routes_polygon_only_1826.json': {'content': json.dumps(route_web)}} #json.dumps(geojson)\n",
    "gist = ghapi.create_gist('', files, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import github3\n",
    "ghapi = github3.login('andy.sy.hwang@gmail.com', password='') #github3.GitHub() \n",
    "files = {'polylines_6routes_expandingroutes buffer(0.00005).json': {'content': json.dumps(line_poly)}} #json.dumps(geojson)  #json.dumps(route_web) if you want multil line plus polygon\n",
    "gist = ghapi.create_gist('', files, public=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geojsonio\n",
    "geojsonio.display(json.dumps(route_web))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import folium\n",
    "# from IPython.display import display\n",
    "\n",
    "# #gjson = hidro.to_crs(epsg='4326').to_json()\n",
    "# mapa = folium.Map([25.67, -81.866667],\n",
    "#                   zoom_start=4,\n",
    "#                   tiles='cartodbpositron')\n",
    "# # looks like this gjson is working by each route only (not working if I put everything in one shot)\n",
    "# # Maybe I should do the same thing for stops in polygon..? by stops for each route..?\n",
    "# # Also, why can't I change the color for each route..? Find a reason!\n",
    "# # Lastly, find if you can graph everything in just 1 geojson fil (it worked in local machine but why not in Jupyter..? Weird)\n",
    "\n",
    "# #2020-07-01\n",
    "# # For map, maybe I can add parameters that users choose in the map -- diameter for stops and buffer size for route (polygon size) , \n",
    "# ## routeID, users (meaning show all the markers/icons that are within the stops) who are within Stops, users who are within Route, users who are not within stops, users who are not within Route and etc\n",
    "# ## Also, if I can, try to add Bus image as icon/marker for bus stops (at the center of the diameter for stop polygon)\n",
    "\n",
    "# points = folium.features.GeoJson(route_web)\n",
    "\n",
    "# mapa.add_child(points)\n",
    "\n",
    "# display(mapa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rander map in ipyleaflet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipyleaflet import GeoJSON\n",
    "# from ipyleaflet import Map, basemaps\n",
    "\n",
    "# geo = GeoJSON(data=route_web)\n",
    "# # First is latitude and second is longitude; both in degrees\n",
    "# center = (25.768947999991482, -80.35031706845287)\n",
    "\n",
    "# map = Map(center=center, interpolation='nearest', basemap=basemaps.Stamen.Terrain)\n",
    "\n",
    "# map.add_layer(geo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets.embed import embed_minimal_html\n",
    "\n",
    "embed_minimal_html('carpentries_instructors_basic.html', views=[map], title='The Carpentries Instructors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rander map in GoogleMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gmaps\n",
    "# import gmaps.datasets\n",
    "# gmaps.configure(api_key=\"AI...\") # Your Google API key\n",
    "\n",
    "# # load a Numpy array of (latitude, longitude) pairs\n",
    "# locations = gmaps.datasets.load_dataset(\"taxi_rides\")\n",
    "\n",
    "# fig = gmaps.figure()\n",
    "# fig.add_layer(gmaps.heatmap_layer(locations))\n",
    "# fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
